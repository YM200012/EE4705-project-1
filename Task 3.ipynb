{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f6e4056-2243-4539-8126-22f357bbcd14",
   "metadata": {},
   "source": [
    "# Task 3: Understand human gesture and body language based on your own built dataset and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6839b1-5947-4fbb-a59c-ec9115a8e650",
   "metadata": {},
   "source": [
    "## 1. Do literature search on dataset building and other deep learning based models applied on gesture recognition. Comment on their applications and benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254603d7-3f27-4e9e-9d29-7df04c362bda",
   "metadata": {},
   "source": [
    "## 2. In the earlier two tasks, you have learned how to do the gesture classification task using the given dataset. Now, you need to collect data by yourself and build your own dataset. The dataset is not limited to gestures. Postures and behavior are encouraged. Please place your data referring to the format of the given dataset. For good performance, the number of data in each class is recommended over 50. For the number of classes, it is better to have more than 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d1de2-a7ef-464a-8386-689961b7e594",
   "metadata": {},
   "source": [
    "## 3. Design your own neural network architecture. Fully connected or convolutional layers used in the first two tasks is acceptable. But you are encouraged to learn more deep learning models and achieve it as possible as you can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f85ae6a-2551-4fd8-ba73-c6b7b3432081",
   "metadata": {},
   "source": [
    "## 3. Write down the problems you encountered during the experiment, the solutions, and your experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0a684964-b989-41c9-978d-c892f6ddd100",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils_data\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4ba23c87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = './own/images'\n",
    "path_processed = './own_processed/images'\n",
    "\n",
    "\n",
    "# -------------------images processing--------------\n",
    "for mainDir, subDir, fileList in os.walk(path):\n",
    "    for file in fileList:\n",
    "        currentPath = os.path.join(mainDir, file)\n",
    "        if file != '.DS_Store':\n",
    "            original = cv2.imread(filename=currentPath)\n",
    "            processedImage = cv2.resize(original, (96,96))\n",
    "\n",
    "            new_mainDir = path_processed + mainDir.split(path)[-1]\n",
    "            if not os.path.exists(new_mainDir):\n",
    "                os.makedirs(new_mainDir)\n",
    "            cv2.imwrite(os.path.join(new_mainDir, file), processedImage)\n",
    "\n",
    "# -----------------label generation----------------\n",
    "label_path = './own_processed/labels'\n",
    "if not os.path.exists(label_path):\n",
    "    os.makedirs(label_path)\n",
    "\n",
    "files = os.listdir(path)\n",
    "index = 0\n",
    "for i, file in enumerate(files):\n",
    "    if file != '.DS_Store':\n",
    "        subclass_label_path = os.path.join(label_path, file + '.txt')\n",
    "        with open(subclass_label_path, 'w') as f:\n",
    "            f.write('#label\\n')\n",
    "        for _ in range(len(os.listdir(os.path.join(path_processed, file)))):\n",
    "            with open(subclass_label_path, 'a') as f:\n",
    "                f.write('{:d}\\n'.format(index))\n",
    "        index = index + 1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "53dc7439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206, 3, 96, 96)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "Image = []\n",
    "path_images = './own_processed/images'\n",
    "for mainDir, subDir, fileList in os.walk(path_images):\n",
    "    for file in fileList:\n",
    "        currentPath = os.path.join(mainDir, file)\n",
    "        Image.append(cv2.resize(cv2.imread(currentPath), (96, 96)))\n",
    "Image = np.array(Image)\n",
    "Image = np.transpose(Image, (0, 3, 1, 2))\n",
    "dataset_size, C, H, W = Image.shape\n",
    "# for FCNN model, the image need to be stretched into one dimension: (b, h, w)->(b, h*w)\n",
    "print (Image.shape)\n",
    "\n",
    "\n",
    "Label = []\n",
    "path_labels = './own_processed/labels'\n",
    "for file in os.listdir(path_labels):\n",
    "    Label.append(np.loadtxt(os.path.join(path_labels, file)))\n",
    "Label = np.array(list(itertools.chain.from_iterable(Label)))\n",
    "num_classes = int(np.max(Label))+1\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "79606f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2595f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is ready!\n"
     ]
    }
   ],
   "source": [
    "dataset = utils_data.TensorDataset(torch.Tensor(Image), torch.LongTensor(Label))\n",
    "split_ratio = 0.8\n",
    "train_size = int(split_ratio * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_set, test_set = utils_data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = utils_data.DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "test_loader = utils_data.DataLoader(dataset=test_set, batch_size=32, shuffle=True)\n",
    "print('Data is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7cd06191",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)  # Batch Normalization\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0)  # Dropout\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Batch Normalization\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0)  # Dropout\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # Batch Normalization\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout3 = nn.Dropout(0)  # Dropout\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)  # Batch Normalization\n",
    "        self.relu4 = nn.ReLU()\n",
    "        #self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout4 = nn.Dropout(0)  # Dropout\n",
    "\n",
    "        self.fc = nn.Linear(128 * 12 * 12, 512)  # Adjust based on final spatial dimensions\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout5 = nn.Dropout(0)  # Dropout\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)  # Adjust based on final spatial dimensions\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.dropout6 = nn.Dropout(0)  # Dropout\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        #x = self.pool3(x)\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.dropout5(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.dropout6(x)\n",
    "        return x\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class CNNRNN(nn.Module):\n",
    "    def __init__(self, cnn, rnn):\n",
    "        super(CNNRNN, self).__init__()\n",
    "        self.cnn = cnn\n",
    "        self.rnn = rnn\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for frame in x:\n",
    "            frame = frame.unsqueeze(0)  # Add batch dimension\n",
    "            features.append(self.cnn(frame))\n",
    "        features = torch.stack(features, dim=0)  # Stack features along time axis\n",
    "        out = self.rnn(features)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0cc4e05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN()\n",
    "rnn = RNN(input_size=256, hidden_size=256, num_layers=2, num_classes=4)\n",
    "model = CNNRNN(cnn, rnn).to(device)\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3a618755",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\ttrain loss=1.364523\ttrain accuracy=0.299\ttest accuracy=0.500\n",
      "epoch=1\ttrain loss=1.028064\ttrain accuracy=0.616\ttest accuracy=0.548\n",
      "epoch=2\ttrain loss=0.791059\ttrain accuracy=0.689\ttest accuracy=0.857\n",
      "epoch=3\ttrain loss=0.582236\ttrain accuracy=0.841\ttest accuracy=0.762\n",
      "epoch=4\ttrain loss=0.443226\ttrain accuracy=0.835\ttest accuracy=0.881\n",
      "epoch=5\ttrain loss=0.316845\ttrain accuracy=0.896\ttest accuracy=0.881\n",
      "epoch=6\ttrain loss=0.310204\ttrain accuracy=0.890\ttest accuracy=0.905\n",
      "epoch=7\ttrain loss=0.312252\ttrain accuracy=0.878\ttest accuracy=0.905\n",
      "epoch=8\ttrain loss=0.279663\ttrain accuracy=0.902\ttest accuracy=0.762\n",
      "epoch=9\ttrain loss=0.245221\ttrain accuracy=0.902\ttest accuracy=0.905\n",
      "epoch=10\ttrain loss=0.233355\ttrain accuracy=0.921\ttest accuracy=0.857\n",
      "epoch=11\ttrain loss=0.241722\ttrain accuracy=0.915\ttest accuracy=0.619\n",
      "epoch=12\ttrain loss=0.286038\ttrain accuracy=0.890\ttest accuracy=0.857\n",
      "epoch=13\ttrain loss=0.243379\ttrain accuracy=0.921\ttest accuracy=0.905\n",
      "epoch=14\ttrain loss=0.223471\ttrain accuracy=0.921\ttest accuracy=0.881\n",
      "epoch=15\ttrain loss=0.202678\ttrain accuracy=0.921\ttest accuracy=0.905\n",
      "epoch=16\ttrain loss=0.260927\ttrain accuracy=0.927\ttest accuracy=0.905\n",
      "epoch=17\ttrain loss=0.207081\ttrain accuracy=0.927\ttest accuracy=0.929\n",
      "epoch=18\ttrain loss=0.295048\ttrain accuracy=0.909\ttest accuracy=0.929\n",
      "epoch=19\ttrain loss=0.176813\ttrain accuracy=0.939\ttest accuracy=0.905\n",
      "epoch=20\ttrain loss=0.213465\ttrain accuracy=0.927\ttest accuracy=0.905\n",
      "epoch=21\ttrain loss=0.191539\ttrain accuracy=0.933\ttest accuracy=0.857\n",
      "epoch=22\ttrain loss=0.167007\ttrain accuracy=0.939\ttest accuracy=0.905\n",
      "epoch=23\ttrain loss=0.162447\ttrain accuracy=0.939\ttest accuracy=0.905\n",
      "epoch=24\ttrain loss=0.150831\ttrain accuracy=0.939\ttest accuracy=0.905\n",
      "epoch=25\ttrain loss=0.153032\ttrain accuracy=0.939\ttest accuracy=0.881\n",
      "epoch=26\ttrain loss=0.225934\ttrain accuracy=0.939\ttest accuracy=0.905\n",
      "epoch=27\ttrain loss=0.192883\ttrain accuracy=0.939\ttest accuracy=0.881\n",
      "epoch=28\ttrain loss=0.224066\ttrain accuracy=0.933\ttest accuracy=0.905\n",
      "epoch=29\ttrain loss=0.203502\ttrain accuracy=0.951\ttest accuracy=0.929\n",
      "epoch=30\ttrain loss=0.142781\ttrain accuracy=0.945\ttest accuracy=0.905\n",
      "epoch=31\ttrain loss=0.195525\ttrain accuracy=0.945\ttest accuracy=0.905\n",
      "epoch=32\ttrain loss=0.121602\ttrain accuracy=0.951\ttest accuracy=0.881\n",
      "epoch=33\ttrain loss=0.193078\ttrain accuracy=0.951\ttest accuracy=0.905\n",
      "epoch=34\ttrain loss=0.117199\ttrain accuracy=0.957\ttest accuracy=0.881\n",
      "epoch=35\ttrain loss=0.104576\ttrain accuracy=0.957\ttest accuracy=0.881\n",
      "epoch=36\ttrain loss=0.098195\ttrain accuracy=0.957\ttest accuracy=0.881\n",
      "epoch=37\ttrain loss=0.108889\ttrain accuracy=0.957\ttest accuracy=0.881\n",
      "epoch=38\ttrain loss=0.106344\ttrain accuracy=0.957\ttest accuracy=0.905\n",
      "epoch=39\ttrain loss=0.100337\ttrain accuracy=0.957\ttest accuracy=0.857\n",
      "epoch=40\ttrain loss=0.089232\ttrain accuracy=0.957\ttest accuracy=0.857\n",
      "epoch=41\ttrain loss=0.078967\ttrain accuracy=0.957\ttest accuracy=0.857\n",
      "epoch=42\ttrain loss=0.070536\ttrain accuracy=0.957\ttest accuracy=0.857\n",
      "epoch=43\ttrain loss=0.065028\ttrain accuracy=0.957\ttest accuracy=0.857\n",
      "epoch=44\ttrain loss=0.126816\ttrain accuracy=0.957\ttest accuracy=0.500\n",
      "epoch=45\ttrain loss=0.493557\ttrain accuracy=0.896\ttest accuracy=0.786\n",
      "epoch=46\ttrain loss=0.390190\ttrain accuracy=0.872\ttest accuracy=0.786\n",
      "epoch=47\ttrain loss=0.575101\ttrain accuracy=0.768\ttest accuracy=0.738\n",
      "epoch=48\ttrain loss=0.313610\ttrain accuracy=0.884\ttest accuracy=0.786\n",
      "epoch=49\ttrain loss=0.308190\ttrain accuracy=0.890\ttest accuracy=0.905\n",
      "epoch=50\ttrain loss=0.230227\ttrain accuracy=0.915\ttest accuracy=0.905\n",
      "epoch=51\ttrain loss=0.207659\ttrain accuracy=0.927\ttest accuracy=0.905\n",
      "epoch=52\ttrain loss=0.182823\ttrain accuracy=0.927\ttest accuracy=0.905\n",
      "epoch=53\ttrain loss=0.188731\ttrain accuracy=0.927\ttest accuracy=0.905\n",
      "epoch=54\ttrain loss=0.181225\ttrain accuracy=0.927\ttest accuracy=0.905\n",
      "epoch=55\ttrain loss=0.172654\ttrain accuracy=0.927\ttest accuracy=0.905\n",
      "epoch=56\ttrain loss=0.174205\ttrain accuracy=0.927\ttest accuracy=0.905\n",
      "epoch=57\ttrain loss=0.203103\ttrain accuracy=0.927\ttest accuracy=0.905\n",
      "epoch=58\ttrain loss=0.177228\ttrain accuracy=0.927\ttest accuracy=0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "test1 = np.zeros(500)\n",
    "loss1 = np.zeros(500)\n",
    "for epoch in range(500):\n",
    "    running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    for step, (batch_image, batch_label) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        #device = torch.device(\"mps\")\n",
    "        batch_image, batch_label = batch_image.to(device), batch_label.to(device)\n",
    "        #batch_output = batch_output.to(device)\n",
    "        batch_output = model(batch_image)\n",
    "        batch_loss = loss_func(batch_output, batch_label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += batch_loss.item()\n",
    "\n",
    "        # train accuracy\n",
    "        _, train_predicted = torch.max(batch_output.data, 1)\n",
    "        train_acc += (train_predicted == batch_label).sum().item()\n",
    "\n",
    "    train_acc /= train_size\n",
    "    running_loss /= (step+1)\n",
    "\n",
    "    # ----------test----------\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    for test_image, test_label in test_loader:\n",
    "        #device = torch.device(\"mps\")\n",
    "        test_image, test_label = test_image.to(device), test_label.to(device)\n",
    "        test_output = model(test_image)\n",
    "        _, predicted = torch.max(test_output.data, 1)\n",
    "        test_acc += (predicted == test_label).sum().item()\n",
    "    test_acc /= test_size\n",
    "\n",
    "    print('epoch={:d}\\ttrain loss={:.6f}\\ttrain accuracy={:.3f}\\ttest accuracy={:.3f}'.format(\n",
    "        epoch, running_loss, train_acc, test_acc))\n",
    "    test1[epoch] = test_acc\n",
    "    loss1[epoch] = running_loss\n",
    "\n",
    "    if test_acc >= best_accuracy:\n",
    "        torch.save(model.state_dict(), './RNN_model.pkl')\n",
    "        best_accuracy = test_acc\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(test1,'red')\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(loss1,'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19779579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
